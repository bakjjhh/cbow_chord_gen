{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2q4dNFPLcPJNFUV3v8/XJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bakjjhh/cbow_chord_gen/blob/main/cbow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVF2HCuero8m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPPQX_orApLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# CBOW 모델 정의\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)  # (batch_size, context_size, embedding_dim)\n",
        "        embeds_mean = torch.mean(embeds, dim=1)  # (batch_size, embedding_dim)\n",
        "        out = self.linear1(embeds_mean)\n",
        "        out = torch.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        log_probs = torch.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "# 학습 데이터 준비\n",
        "CONTEXT_SIZE = 2  # 주변 단어의 개수\n",
        "raw_text = \"I am learning PyTorch for natural language processing\".split()\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i-2], raw_text[i-1], raw_text[i+1], raw_text[i+2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "# 단어와 인덱스 사전 생성\n",
        "vocab = set(raw_text)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "model = CBOW(vocab_size, embedding_dim, CONTEXT_SIZE)\n",
        "\n",
        "# 손실 함수와 옵티마이저 설정\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 진행\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        # 입력 데이터 준비\n",
        "        context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        # 모델의 forward 연산 및 손실 계산\n",
        "        model.zero_grad()\n",
        "        log_probs = model(context_idxs)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long))\n",
        "\n",
        "        # 역전파 및 가중치 업데이트\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print('Epoch:', epoch, 'Loss:', total_loss)\n",
        "\n",
        "# 임베딩 벡터 출력\n",
        "embedding_weights = model.embeddings.weight.data\n",
        "for i, word in enumerate(vocab):\n",
        "    print('Word:', word, 'Embedding:', embedding_weights[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6DNfJwrsFHj",
        "outputId": "1db1cbe2-2cde-440d-c57d-48008f4ac5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 8.326865077018738\n",
            "Epoch: 1 Loss: 8.290189266204834\n",
            "Epoch: 2 Loss: 8.253683805465698\n",
            "Epoch: 3 Loss: 8.217411637306213\n",
            "Epoch: 4 Loss: 8.181631088256836\n",
            "Epoch: 5 Loss: 8.145939707756042\n",
            "Epoch: 6 Loss: 8.110459089279175\n",
            "Epoch: 7 Loss: 8.07521903514862\n",
            "Epoch: 8 Loss: 8.040315747261047\n",
            "Epoch: 9 Loss: 8.005446195602417\n",
            "Epoch: 10 Loss: 7.970909237861633\n",
            "Epoch: 11 Loss: 7.936586380004883\n",
            "Epoch: 12 Loss: 7.902398228645325\n",
            "Epoch: 13 Loss: 7.868391394615173\n",
            "Epoch: 14 Loss: 7.834531664848328\n",
            "Epoch: 15 Loss: 7.80071222782135\n",
            "Epoch: 16 Loss: 7.7666285037994385\n",
            "Epoch: 17 Loss: 7.732800245285034\n",
            "Epoch: 18 Loss: 7.699065923690796\n",
            "Epoch: 19 Loss: 7.665529251098633\n",
            "Epoch: 20 Loss: 7.632246971130371\n",
            "Epoch: 21 Loss: 7.5990952253341675\n",
            "Epoch: 22 Loss: 7.56603741645813\n",
            "Epoch: 23 Loss: 7.5330469608306885\n",
            "Epoch: 24 Loss: 7.500448942184448\n",
            "Epoch: 25 Loss: 7.467903017997742\n",
            "Epoch: 26 Loss: 7.435391068458557\n",
            "Epoch: 27 Loss: 7.403200268745422\n",
            "Epoch: 28 Loss: 7.3711360692977905\n",
            "Epoch: 29 Loss: 7.339208245277405\n",
            "Epoch: 30 Loss: 7.3073378801345825\n",
            "Epoch: 31 Loss: 7.275973558425903\n",
            "Epoch: 32 Loss: 7.244700193405151\n",
            "Epoch: 33 Loss: 7.213651776313782\n",
            "Epoch: 34 Loss: 7.182788729667664\n",
            "Epoch: 35 Loss: 7.15213930606842\n",
            "Epoch: 36 Loss: 7.121636509895325\n",
            "Epoch: 37 Loss: 7.091248035430908\n",
            "Epoch: 38 Loss: 7.060683369636536\n",
            "Epoch: 39 Loss: 7.030634164810181\n",
            "Epoch: 40 Loss: 7.00036883354187\n",
            "Epoch: 41 Loss: 6.970345377922058\n",
            "Epoch: 42 Loss: 6.940610766410828\n",
            "Epoch: 43 Loss: 6.910742163658142\n",
            "Epoch: 44 Loss: 6.881098508834839\n",
            "Epoch: 45 Loss: 6.851574897766113\n",
            "Epoch: 46 Loss: 6.822162985801697\n",
            "Epoch: 47 Loss: 6.792788505554199\n",
            "Epoch: 48 Loss: 6.763612747192383\n",
            "Epoch: 49 Loss: 6.734518766403198\n",
            "Epoch: 50 Loss: 6.705484986305237\n",
            "Epoch: 51 Loss: 6.676542282104492\n",
            "Epoch: 52 Loss: 6.6476229429244995\n",
            "Epoch: 53 Loss: 6.619113206863403\n",
            "Epoch: 54 Loss: 6.590582489967346\n",
            "Epoch: 55 Loss: 6.562296628952026\n",
            "Epoch: 56 Loss: 6.5338135957717896\n",
            "Epoch: 57 Loss: 6.505730986595154\n",
            "Epoch: 58 Loss: 6.477537393569946\n",
            "Epoch: 59 Loss: 6.449626088142395\n",
            "Epoch: 60 Loss: 6.421573638916016\n",
            "Epoch: 61 Loss: 6.393755674362183\n",
            "Epoch: 62 Loss: 6.3660606145858765\n",
            "Epoch: 63 Loss: 6.33845591545105\n",
            "Epoch: 64 Loss: 6.31123149394989\n",
            "Epoch: 65 Loss: 6.284067869186401\n",
            "Epoch: 66 Loss: 6.25669002532959\n",
            "Epoch: 67 Loss: 6.229679226875305\n",
            "Epoch: 68 Loss: 6.20250928401947\n",
            "Epoch: 69 Loss: 6.175821900367737\n",
            "Epoch: 70 Loss: 6.148632764816284\n",
            "Epoch: 71 Loss: 6.121948957443237\n",
            "Epoch: 72 Loss: 6.095074892044067\n",
            "Epoch: 73 Loss: 6.068622708320618\n",
            "Epoch: 74 Loss: 6.041921138763428\n",
            "Epoch: 75 Loss: 6.015470623970032\n",
            "Epoch: 76 Loss: 5.988799810409546\n",
            "Epoch: 77 Loss: 5.962696313858032\n",
            "Epoch: 78 Loss: 5.936182260513306\n",
            "Epoch: 79 Loss: 5.910095810890198\n",
            "Epoch: 80 Loss: 5.883766531944275\n",
            "Epoch: 81 Loss: 5.857752680778503\n",
            "Epoch: 82 Loss: 5.831766486167908\n",
            "Epoch: 83 Loss: 5.805652141571045\n",
            "Epoch: 84 Loss: 5.779998302459717\n",
            "Epoch: 85 Loss: 5.753913879394531\n",
            "Epoch: 86 Loss: 5.728387475013733\n",
            "Epoch: 87 Loss: 5.702547907829285\n",
            "Epoch: 88 Loss: 5.6768070459365845\n",
            "Epoch: 89 Loss: 5.6516512632369995\n",
            "Epoch: 90 Loss: 5.6260035037994385\n",
            "Epoch: 91 Loss: 5.600718379020691\n",
            "Epoch: 92 Loss: 5.5751824378967285\n",
            "Epoch: 93 Loss: 5.550291657447815\n",
            "Epoch: 94 Loss: 5.524974822998047\n",
            "Epoch: 95 Loss: 5.499952673912048\n",
            "Epoch: 96 Loss: 5.474847674369812\n",
            "Epoch: 97 Loss: 5.449929356575012\n",
            "Epoch: 98 Loss: 5.424911260604858\n",
            "Epoch: 99 Loss: 5.400387763977051\n",
            "Word: processing Embedding: tensor([-2.2724, -0.2316,  0.4592, -1.1704,  1.5096,  0.4468, -0.7018,  0.0193,\n",
            "         0.3933, -0.3178,  2.7357, -0.3253,  0.2754,  0.1619, -1.3241,  0.3749,\n",
            "        -0.3755, -1.9854,  0.9958, -1.0222,  0.9912, -0.6548,  0.8668,  0.2734,\n",
            "         0.5304,  0.4687,  0.6645, -0.6684,  0.7375,  2.5093,  0.0320, -0.5364,\n",
            "        -0.2634,  0.5128,  0.6210, -1.1683, -0.6946, -0.6308, -0.2183,  0.2714,\n",
            "         1.3873,  0.3792,  0.7747, -0.3048,  0.9087,  0.4971, -0.3388, -1.8120,\n",
            "        -1.1570,  0.4990, -1.2404,  0.9479,  0.5274, -2.2727, -0.2662,  0.6731,\n",
            "         0.2073,  1.2846, -0.4735, -0.3169, -0.0443, -0.9000, -0.9271,  0.2951,\n",
            "        -1.3057, -1.5547,  1.3032,  1.2292, -2.2950, -0.3631, -0.2808, -0.5694,\n",
            "        -0.2708, -0.3300,  0.3039, -1.1436,  1.1840,  0.1101, -0.5731,  0.4186,\n",
            "        -0.5908,  2.2209,  0.7900,  0.4192,  0.4440, -1.4746,  1.2035,  0.6951,\n",
            "        -0.8506, -0.6771,  0.5417, -0.5357,  0.9588,  0.3685,  1.3050,  1.0696,\n",
            "        -1.1088, -0.8899,  0.5477, -0.5395])\n",
            "Word: PyTorch Embedding: tensor([-0.1001, -0.1787,  0.0346, -0.7099, -2.4745,  1.9369, -1.1885,  0.4598,\n",
            "        -0.4113, -0.3829,  1.0458,  1.1439, -1.5384,  1.5626,  0.4995,  0.5130,\n",
            "        -0.2388, -1.0685,  1.2032,  0.5752,  0.4452,  0.6071, -1.4353, -0.4224,\n",
            "        -0.0577,  0.9203,  0.8764, -0.0181, -0.4503,  0.1622, -0.0747, -1.6264,\n",
            "        -0.4740,  0.7656, -0.2980,  1.0058, -0.5823, -0.7918,  0.1574,  0.2662,\n",
            "        -0.0692,  1.2732,  0.2634,  1.0690,  0.8170,  0.9376,  0.7013,  1.5820,\n",
            "         1.6215, -0.2171, -0.0955,  0.0290,  1.1638, -0.6848,  0.1332,  1.0167,\n",
            "        -0.1562,  0.7633,  0.2193,  0.4899,  0.2904,  0.6549, -0.3317,  0.6389,\n",
            "        -0.5930,  1.4689,  1.2674, -0.9158,  1.2696,  0.6730, -1.2326, -1.9365,\n",
            "        -2.5190,  1.0459, -1.3382, -1.0314, -1.7841,  0.2294,  0.8063, -0.2208,\n",
            "        -0.1563, -1.0194,  0.0508, -1.0991, -0.2455, -0.8328, -2.3328, -1.8334,\n",
            "         0.2627, -2.3182, -1.0549,  1.0308,  0.9712, -1.0712, -0.3069,  0.1574,\n",
            "         0.1896, -1.6273, -0.8409,  0.9075])\n",
            "Word: learning Embedding: tensor([ 0.2787,  1.0108,  1.1805,  0.5528, -0.4236,  0.9405, -0.8520,  0.2542,\n",
            "         0.3791,  0.6286,  0.0937, -1.5353, -0.5569,  1.1697, -0.1756,  1.6318,\n",
            "        -1.1565, -0.9561,  2.7176,  0.7889, -0.1694, -0.1212,  0.6538,  0.0515,\n",
            "         0.4563, -0.8221,  1.8702, -1.3927, -0.1027, -0.2114,  0.1942, -0.7976,\n",
            "         0.2822, -1.0887,  0.3505,  0.3481, -0.1493, -0.4175,  0.2874,  0.3847,\n",
            "         1.5082,  0.1512,  1.3371, -0.2163, -0.1023, -0.8470,  0.0649, -0.3064,\n",
            "         0.6808, -0.4542,  1.0984, -0.6708,  0.3007,  0.0806,  0.7657, -0.3037,\n",
            "        -1.4157, -1.6521, -1.9897,  0.4598,  0.7673,  0.9921,  2.0593,  1.1451,\n",
            "         0.7860,  0.0222, -1.6225,  2.4505, -2.6391, -0.3046, -0.2494, -1.0251,\n",
            "         1.0081,  0.2723,  1.2750,  0.4156,  0.5652,  0.6295, -1.2973,  0.0414,\n",
            "         1.4925,  0.1862,  0.1317, -0.0579,  0.3847, -0.0518, -1.0798,  0.1794,\n",
            "         0.5976,  2.2702,  0.8322,  0.2279,  0.1898,  1.1376, -0.8991,  2.3106,\n",
            "        -1.0802, -0.3782,  1.1668, -0.5839])\n",
            "Word: for Embedding: tensor([ 0.0784, -0.7935, -0.2502, -0.8565, -0.8539,  0.8438, -1.0951, -1.1141,\n",
            "         1.4681,  2.0138, -1.5961, -1.4327,  0.2109, -0.3171, -0.6776, -1.0542,\n",
            "         0.5451,  1.5078,  0.7005, -0.3185,  1.0448, -1.7766,  2.0921, -0.7768,\n",
            "        -0.9265, -0.5893,  0.1924, -0.6893, -0.6162, -0.1583, -0.3334, -1.6473,\n",
            "        -0.7906, -0.7581, -0.1630, -0.1187,  1.0921, -1.4082,  1.2562,  1.0113,\n",
            "        -0.8568, -1.3172, -0.2616,  0.0744,  0.0073, -2.0436, -0.2091, -0.0547,\n",
            "         0.5541,  0.9350, -0.6878, -1.2201,  0.3904, -0.3669, -0.6069, -1.1459,\n",
            "         0.3271,  0.9157,  1.1357, -0.3310, -0.5999,  0.8124,  0.0732, -0.3568,\n",
            "        -0.2215, -0.3364,  1.5456,  1.1955,  0.3884, -0.6149, -0.2072,  0.2488,\n",
            "        -1.1328,  1.2828, -1.6106, -2.1094, -0.1505,  0.3608,  0.6325,  0.3008,\n",
            "        -0.9491,  1.8471, -0.7212,  0.5633,  0.2134, -1.5168, -0.0947,  1.2954,\n",
            "        -0.9770,  0.1816, -1.0190,  0.5281,  1.1386, -0.3336,  0.9227,  1.4389,\n",
            "        -1.4901, -1.4757, -0.4053, -2.2224])\n",
            "Word: language Embedding: tensor([-0.1604,  0.1265,  0.7686,  1.8287, -0.8075, -0.6658, -0.3829,  0.6247,\n",
            "        -0.6415,  1.0546,  0.7659, -0.1811, -0.3685, -0.4605,  0.0308,  0.0439,\n",
            "         1.4548,  0.2399, -0.0859,  0.7644,  2.1411,  0.6678, -0.7926, -0.0770,\n",
            "         0.4238,  0.4659, -1.6992, -2.0527,  1.6519,  1.4159, -0.3896,  0.3089,\n",
            "        -0.0865, -0.5262, -0.4728, -0.5675,  0.5924,  0.8306, -0.2625, -0.1093,\n",
            "         0.6709,  1.0768, -0.6541, -1.4626, -2.5742, -0.2878,  0.6236, -1.2681,\n",
            "        -0.1731, -1.5248,  0.1838,  0.2335,  1.8741,  0.3871,  1.3915, -0.0031,\n",
            "         0.0078,  1.6258,  0.0880, -0.2805, -1.0830,  1.5248,  0.7662,  1.6744,\n",
            "        -0.7665,  1.4730, -0.8220,  0.5825, -0.0432, -0.5062,  2.6603, -0.7710,\n",
            "         0.0300,  1.0799,  1.6741, -2.2875,  2.5260, -1.9870, -0.0077, -0.6559,\n",
            "        -1.8201, -0.8449,  0.2695, -0.6942, -0.3829, -0.2245,  1.3128, -1.0670,\n",
            "         0.5376,  0.0183, -0.3441,  2.7831, -0.1438,  0.4141,  0.6841,  1.6418,\n",
            "        -0.1902, -1.9740,  1.3902, -0.4183])\n",
            "Word: natural Embedding: tensor([ 0.3405, -2.0619, -1.0038,  0.7025, -1.1469, -2.0851, -1.5468, -0.6366,\n",
            "         2.1169, -0.6706, -0.5438, -0.3346, -0.6646, -0.9691, -1.2799,  1.0231,\n",
            "         0.7284,  0.5946, -0.4674,  0.3080,  0.3870, -1.6687,  0.3031,  0.4593,\n",
            "        -0.5861,  0.2413, -1.1762, -0.5335,  0.4879, -1.3492,  0.5926,  0.4992,\n",
            "         0.5264,  0.3131, -0.8111,  0.3766, -2.1854,  0.5117, -0.3398,  0.6532,\n",
            "         0.3227,  0.4184, -2.7089,  0.2124, -1.6385,  0.2193, -2.0854,  0.9421,\n",
            "         0.0812, -0.8364,  0.2708,  0.7625, -2.7586, -0.3058, -0.3580,  1.8050,\n",
            "         0.9768, -1.5261,  0.2172,  0.1511, -0.0571,  1.5386, -0.0405, -0.4066,\n",
            "         0.6632, -0.6419, -1.4284, -0.7717,  0.7240,  1.0898,  0.3177,  0.2760,\n",
            "        -1.2990,  0.0221, -0.1650,  1.0474, -0.1281, -0.9483,  0.7316, -1.6657,\n",
            "         0.8120,  1.3668, -0.4107,  0.4290,  1.0616,  1.4486, -1.5431,  1.3981,\n",
            "        -0.4296,  1.1963, -0.7177,  1.0984, -0.5345, -3.1764, -0.8035, -0.0941,\n",
            "        -0.3085, -1.5749,  0.8425,  0.6640])\n",
            "Word: I Embedding: tensor([-1.7502,  0.1127,  0.6817, -0.3283, -0.3240,  0.8284,  0.3033,  0.5391,\n",
            "        -0.2480, -1.3744,  2.3083,  0.7899, -0.4075,  0.3101, -1.5946, -1.5405,\n",
            "         0.1368,  0.7522,  0.9173, -0.8073, -0.7737, -0.5608,  0.1759,  1.6302,\n",
            "         1.7085, -0.2124,  0.9765,  1.3718,  0.1848,  2.4504,  0.4446,  0.5397,\n",
            "        -0.5107, -0.2436,  0.5853,  0.8569, -2.3886, -0.0106,  0.4622, -0.4960,\n",
            "        -2.4320, -0.0359, -1.8794,  1.4960, -0.3300, -0.0285, -0.1460,  0.5892,\n",
            "         1.9115,  0.7983,  0.2491,  0.2846, -0.9798, -0.7838, -1.0693, -1.2761,\n",
            "        -0.6012,  0.5384,  0.7852, -0.5988, -0.9594, -0.2274, -0.4409, -0.2878,\n",
            "         1.9409,  0.6516, -1.1285,  0.5049, -0.3428, -0.0171, -0.5421, -0.8157,\n",
            "         1.0885,  1.0010,  0.7411, -0.9575, -0.1926, -0.8072,  0.1117,  0.2703,\n",
            "         0.4058, -0.0429,  0.1662,  1.1806,  0.0476,  1.0115,  0.8008, -0.5607,\n",
            "        -0.5410, -0.4525,  1.7670, -0.9675,  0.3645, -0.6434, -0.5830, -0.8375,\n",
            "         0.2335, -0.1715, -0.6437, -2.0085])\n",
            "Word: am Embedding: tensor([-0.6143,  0.5831,  2.1230,  1.1134, -0.4807, -1.3434,  0.4638,  0.5669,\n",
            "         0.7549, -1.1185, -0.9346, -0.5424, -0.4039,  0.9060,  0.1269, -1.8932,\n",
            "        -0.7994,  0.2918,  0.2454, -0.3449,  0.7541, -1.8972,  1.4505, -1.0121,\n",
            "         3.0971, -1.1950,  0.0142,  0.8789,  0.0038,  1.1353,  1.2649, -0.6449,\n",
            "        -0.7759, -0.7246,  0.9138,  1.1039,  1.1983,  0.4795, -1.0722, -0.5531,\n",
            "        -0.4530,  1.8893,  0.5983, -0.5422, -0.1726,  1.4750,  0.1660, -0.7723,\n",
            "        -1.4391,  0.9766, -1.5578, -1.3899,  0.6730,  0.0634,  0.7373, -1.0350,\n",
            "         2.3988, -0.6307, -0.4305, -0.4229,  0.6116,  0.1636,  1.7810,  0.9673,\n",
            "        -1.2590, -0.1719,  0.1626, -0.5500,  0.2315, -0.3327, -0.1688, -0.5049,\n",
            "         0.3631, -0.4479, -0.9414, -1.0894, -1.3972, -0.1920,  0.5103, -1.9310,\n",
            "        -0.4630,  2.9674, -0.9697, -1.6867,  1.4857, -2.7357, -0.2259,  0.2577,\n",
            "         1.4349,  0.8024,  0.2644, -0.4516,  1.6708,  1.9300,  0.5812,  0.3022,\n",
            "        -0.8231,  0.1903,  0.5026, -0.1539])\n"
          ]
        }
      ]
    }
  ]
}